{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cc = np.load('cluster_centers.npy')\n",
    "labels = np.load('labels.npy')\n",
    "tac = np.load('train_art_clusters.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 50)\n"
     ]
    }
   ],
   "source": [
    "print cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048575, 3)\n"
     ]
    }
   ],
   "source": [
    "print tac.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233286,)\n"
     ]
    }
   ],
   "source": [
    "print labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['03098741-08b3-4dd7-b3f6-1b0bfa2c879c' 'Liars']\n",
      " ['69c4cc43-8163-41c5-ac81-30946d27bb69' 'CunninLynguists']\n",
      " ['7a2e6b55-f149-4e74-be6a-30a1b1a387bb' 'The Desert Sessions']\n",
      " ..., \n",
      " ['8974da95-e631-45aa-8fd7-aa0c2795f997' 'Harry Gregson-Williams']\n",
      " ['8067c102-4996-42bc-9980-06ce2e644eae' 'Soul Coughing']\n",
      " ['39c2a93d-9afa-4a22-9bba-c087ab056e1c' 'Jefferson Airplane']]\n",
      "(233286, 4)\n"
     ]
    }
   ],
   "source": [
    "# relies on chopping off column headers ahead of time. \n",
    "artists = np.loadtxt('data/artists.csv', dtype='string', delimiter=',')\n",
    "print artists\n",
    "profiles = np.loadtxt('data/profiles_fixed.csv', dtype='string', delimiter=',')\n",
    "print profiles.shape\n",
    "\n",
    "# fixing erroneous, irrelevant ages like 1002\n",
    "max_age = 120\n",
    "# total = 0\n",
    "# count = 0\n",
    "for p in profiles:\n",
    "    if p[2] != '' and (int(p[2]) >= max_age or int(p[2]) < 0):\n",
    "        # 22 is the average age of those who put down ages within a reasonable range\n",
    "        p[2] = '22'\n",
    "\n",
    "art_dict = {}\n",
    "count = 0\n",
    "for a in artists:\n",
    "    art_dict[a[0]] = count\n",
    "    count += 1\n",
    "    \n",
    "prof_dict = {}\n",
    "count = 0\n",
    "for a in profiles:\n",
    "    prof_dict[a[0]] = count\n",
    "    count += 1\n",
    "    \n",
    "nation_dict = {}\n",
    "count = 0\n",
    "for p in profiles:\n",
    "    if p[3] not in nation_dict:\n",
    "        nation_dict[p[3]] = None\n",
    "num_nations = len(nation_dict)\n",
    "count = 0\n",
    "for key in nation_dict.keys():\n",
    "    nation_dict[key] = [0] * num_nations\n",
    "    nation_dict[key][count] = 1\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train = np.loadtxt('data/train_fixed.csv', dtype='string', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_one_hot(length, n):\n",
    "    lst = [0] * length\n",
    "    lst[n] = 1\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pre_X = []\n",
    "pre_Y = []\n",
    "count = 0\n",
    "for t in train[:1048575]:\n",
    "# for t in train[:10]:\n",
    "    p = prof_dict[t[0]]\n",
    "    \n",
    "    temp_x = []\n",
    "    temp_x += make_one_hot(200, labels[p])\n",
    "    temp_x += make_one_hot(50, int(tac[count][1]))\n",
    "    \n",
    "    if profiles[p][1] == 'f':\n",
    "        temp_x += make_one_hot(3, 0)\n",
    "    elif profiles[p][1] == 'm':\n",
    "        temp_x += make_one_hot(3, 1)\n",
    "    else:\n",
    "        temp_x += make_one_hot(3, 2)\n",
    "        \n",
    "    if profiles[p][2] == '':\n",
    "        temp_x += make_one_hot(121, 120)\n",
    "    else:\n",
    "        temp_x += make_one_hot(121, int(profiles[p][2]))\n",
    "        \n",
    "    temp_x += nation_dict[profiles[p][3]]\n",
    "        \n",
    "    pre_X.append(temp_x)\n",
    "    \n",
    "    pre_Y.append(int(t[2]))\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n",
      "[554  81 708 ..., 230  66 284]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(pre_X)\n",
    "Y = np.array(pre_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mse(Y, Y_cv):\n",
    "    diff = Y_cv - Y\n",
    "    d_sq = np.square(diff)\n",
    "    mse_res = np.mean(d_sq)\n",
    "    return mse_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 319097.67500054\n",
      "Iteration 2, loss = 299663.07319329\n",
      "Iteration 3, loss = 294541.12457682\n",
      "Iteration 4, loss = 291878.43829243\n",
      "Iteration 5, loss = 290117.92648822\n",
      "Iteration 6, loss = 288792.43514116\n",
      "Iteration 7, loss = 287775.12603029\n",
      "Iteration 8, loss = 286936.67536880\n",
      "Iteration 9, loss = 286264.29826525\n",
      "Iteration 10, loss = 285683.73858215\n",
      "Iteration 11, loss = 285219.90430750\n",
      "Iteration 12, loss = 284799.68837347\n",
      "Iteration 13, loss = 284444.97253445\n",
      "Iteration 14, loss = 284112.93953250\n",
      "Iteration 15, loss = 283786.50531307\n",
      "Iteration 16, loss = 283518.14733265\n",
      "Iteration 17, loss = 283246.69709349\n",
      "Iteration 18, loss = 282998.42572028\n",
      "Iteration 19, loss = 282761.22661641\n",
      "Iteration 20, loss = 282537.95932575\n",
      "Iteration 21, loss = 282322.33041414\n",
      "Iteration 22, loss = 282131.33000825\n",
      "Iteration 23, loss = 281897.45422551\n",
      "Iteration 24, loss = 281711.91342968\n",
      "Iteration 25, loss = 281548.05709230\n",
      "Iteration 26, loss = 281345.99843318\n",
      "Iteration 27, loss = 281175.87307260\n",
      "Iteration 28, loss = 280998.23125692\n",
      "Iteration 29, loss = 280832.36988492\n",
      "Iteration 30, loss = 280642.92274166\n",
      "Iteration 31, loss = 280502.83785051\n",
      "Iteration 32, loss = 280318.90759802\n",
      "Iteration 33, loss = 280188.22182978\n",
      "Iteration 34, loss = 280009.44091113\n",
      "Iteration 35, loss = 279849.18651614\n",
      "Iteration 36, loss = 279684.22288050\n",
      "Iteration 37, loss = 279535.97570733\n",
      "Iteration 38, loss = 279362.50300453\n",
      "Iteration 39, loss = 279206.27486982\n",
      "Iteration 40, loss = 279077.01280617\n",
      "Iteration 41, loss = 278900.11531581\n",
      "Iteration 42, loss = 278772.86029916\n",
      "Iteration 43, loss = 278581.09688532\n",
      "Iteration 44, loss = 278461.28533014\n",
      "Iteration 45, loss = 278326.46686162\n",
      "Iteration 46, loss = 278139.01023040\n",
      "Iteration 47, loss = 278040.06698121\n",
      "Iteration 48, loss = 277871.07942258\n",
      "Iteration 49, loss = 277721.38928998\n",
      "Iteration 50, loss = 277593.96870064\n",
      "Iteration 51, loss = 277467.38727294\n",
      "Iteration 52, loss = 277319.10138532\n",
      "Iteration 53, loss = 277155.44513778\n",
      "Iteration 54, loss = 277043.52441189\n",
      "Iteration 55, loss = 276913.48548548\n",
      "Iteration 56, loss = 276766.45927956\n",
      "Iteration 57, loss = 276633.08447094\n",
      "Iteration 58, loss = 276507.11865467\n",
      "Iteration 59, loss = 276383.70098981\n",
      "Iteration 60, loss = 276229.40080998\n",
      "Iteration 61, loss = 276125.29156366\n",
      "Iteration 62, loss = 275963.82432369\n",
      "Iteration 63, loss = 275836.87085851\n",
      "Iteration 64, loss = 275710.05598688\n",
      "Iteration 65, loss = 275602.89308468\n",
      "Iteration 66, loss = 275434.73638486\n",
      "Iteration 67, loss = 275341.86818622\n",
      "Iteration 68, loss = 275207.35196131\n",
      "Iteration 69, loss = 275079.89002005\n",
      "Iteration 70, loss = 274982.75477212\n",
      "Iteration 71, loss = 274839.57060253\n",
      "Iteration 72, loss = 274707.71366338\n",
      "Iteration 73, loss = 274562.75359373\n",
      "Iteration 74, loss = 274447.84251256\n",
      "Iteration 75, loss = 274311.79332354\n",
      "Iteration 76, loss = 274209.87531944\n",
      "Iteration 77, loss = 274073.25725711\n",
      "Iteration 78, loss = 273960.19895720\n",
      "Iteration 79, loss = 273861.25877552\n",
      "Iteration 80, loss = 273709.55012200\n",
      "Iteration 81, loss = 273567.04234348\n",
      "Iteration 82, loss = 273452.22617084\n",
      "Iteration 83, loss = 273370.80548568\n",
      "Iteration 84, loss = 273239.11609300\n",
      "Iteration 85, loss = 273120.43004938\n",
      "Iteration 86, loss = 272974.64040690\n",
      "Iteration 87, loss = 272858.74722671\n",
      "Iteration 88, loss = 272780.39721956\n",
      "Iteration 89, loss = 272664.55964280\n",
      "Iteration 90, loss = 272530.69298836\n",
      "Iteration 91, loss = 272441.05118656\n",
      "Iteration 92, loss = 272282.44583949\n",
      "Iteration 93, loss = 272206.96421584\n",
      "Iteration 94, loss = 272107.66795010\n",
      "Iteration 95, loss = 271959.45428596\n",
      "Iteration 96, loss = 271866.34064216\n",
      "Iteration 97, loss = 271706.46045954\n",
      "Iteration 98, loss = 271635.21661793\n",
      "Iteration 99, loss = 271512.19184311\n",
      "Iteration 100, loss = 271403.93920018\n",
      "Iteration 101, loss = 271312.54392477\n",
      "Iteration 102, loss = 271172.61040110\n",
      "Iteration 103, loss = 271085.30689892\n",
      "Iteration 104, loss = 270983.86565229\n",
      "Iteration 105, loss = 270860.48078062\n",
      "Iteration 106, loss = 270760.40649705\n",
      "Iteration 107, loss = 270601.44846290\n",
      "Iteration 108, loss = 270501.14740549\n",
      "Iteration 109, loss = 270408.87680506\n",
      "Iteration 110, loss = 270308.57117733\n",
      "Iteration 111, loss = 270170.39545894\n",
      "Iteration 112, loss = 270059.51090732\n",
      "Iteration 113, loss = 269887.58334023\n",
      "Iteration 114, loss = 269804.02928275\n",
      "Iteration 115, loss = 269710.80475281\n",
      "Iteration 116, loss = 269563.85721086\n",
      "Iteration 117, loss = 269470.30689765\n",
      "Iteration 118, loss = 269371.32098157\n",
      "Iteration 119, loss = 269218.95184266\n",
      "Iteration 120, loss = 269142.08625009\n",
      "Iteration 121, loss = 269055.27086171\n",
      "Iteration 122, loss = 268939.65836374\n",
      "Iteration 123, loss = 268830.72921129\n",
      "Iteration 124, loss = 268740.76447282\n",
      "Iteration 125, loss = 268636.63673456\n",
      "Iteration 126, loss = 268510.12376049\n",
      "Iteration 127, loss = 268411.54930680\n",
      "Iteration 128, loss = 268278.09241915\n",
      "Iteration 129, loss = 268164.64025625\n",
      "Iteration 130, loss = 268037.12994801\n",
      "Iteration 131, loss = 267900.25774659\n",
      "Iteration 132, loss = 267782.63209981\n",
      "Iteration 133, loss = 267685.30569120\n",
      "Iteration 134, loss = 267599.11677102\n",
      "Iteration 135, loss = 267527.15178316\n",
      "Iteration 136, loss = 267378.84327863\n",
      "Iteration 137, loss = 267267.64323319\n",
      "Iteration 138, loss = 267146.74386871\n",
      "Iteration 139, loss = 267067.81462424\n",
      "Iteration 140, loss = 266922.69851822\n",
      "Iteration 141, loss = 266829.64597155\n",
      "Iteration 142, loss = 266736.03699875\n",
      "Iteration 143, loss = 266609.73495391\n",
      "Iteration 144, loss = 266533.40884644\n",
      "Iteration 145, loss = 266401.29846012\n",
      "Iteration 146, loss = 266304.28612462\n",
      "Iteration 147, loss = 266146.07078410\n",
      "Iteration 148, loss = 266074.06785807\n",
      "Iteration 149, loss = 265966.73164759\n",
      "Iteration 150, loss = 265837.29393520\n",
      "Iteration 151, loss = 265725.17277835\n",
      "Iteration 152, loss = 265601.10614889\n",
      "Iteration 153, loss = 265526.83578524\n",
      "Iteration 154, loss = 265413.92118321\n",
      "Iteration 155, loss = 265331.56395134\n",
      "Iteration 156, loss = 265201.80673205\n",
      "Iteration 157, loss = 265060.46997906\n",
      "Iteration 158, loss = 264948.34512833\n",
      "Iteration 159, loss = 264833.94499745\n",
      "Iteration 160, loss = 264708.01010692\n",
      "Iteration 161, loss = 264642.36305479\n",
      "Iteration 162, loss = 264505.96678320\n",
      "Iteration 163, loss = 264400.64477066\n",
      "Iteration 164, loss = 264282.62085182\n",
      "Iteration 165, loss = 264193.60664123\n",
      "Iteration 166, loss = 264087.24980373\n",
      "Iteration 167, loss = 263952.39458214\n",
      "Iteration 168, loss = 263870.13409958\n",
      "Iteration 169, loss = 263785.93386831\n",
      "Iteration 170, loss = 263638.36514385\n",
      "Iteration 171, loss = 263604.81839970\n",
      "Iteration 172, loss = 263481.49199968\n",
      "Iteration 173, loss = 263365.05617868\n",
      "Iteration 174, loss = 263216.07321238\n",
      "Iteration 175, loss = 263132.92759214\n",
      "Iteration 176, loss = 263020.32544546\n",
      "Iteration 177, loss = 262905.68670927\n",
      "Iteration 178, loss = 262818.74516191\n",
      "Iteration 179, loss = 262797.03315752\n",
      "Iteration 180, loss = 262634.71653084\n",
      "Iteration 181, loss = 262538.48957182\n",
      "Iteration 182, loss = 262380.66790802\n",
      "Iteration 183, loss = 262355.11529089\n",
      "Iteration 184, loss = 262205.22822599\n",
      "Iteration 185, loss = 262087.00948158\n",
      "Iteration 186, loss = 262041.39579150\n",
      "Iteration 187, loss = 261969.93744215\n",
      "Iteration 188, loss = 261779.11220678\n",
      "Iteration 189, loss = 261709.36392991\n",
      "Iteration 190, loss = 261628.34144692\n",
      "Iteration 191, loss = 261487.97896651\n",
      "Iteration 192, loss = 261409.70143176\n",
      "Iteration 193, loss = 261376.58666808\n",
      "Iteration 194, loss = 261200.01725487\n",
      "Iteration 195, loss = 261171.43348768\n",
      "Iteration 196, loss = 261026.31159636\n",
      "Iteration 197, loss = 260917.17722119\n",
      "Iteration 198, loss = 260818.96074324\n",
      "Iteration 199, loss = 260776.43258894\n",
      "Iteration 200, loss = 260624.86464994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MLP = neural_network.MLPRegressor(max_iter=200, verbose=True)\n",
    "MLP.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145067.34981\n"
     ]
    }
   ],
   "source": [
    "Y_cv = MLP.predict(X[-1000:])\n",
    "print mse(Y[-1000:], Y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(MLP, open(\"mlp_model.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP = pickle.load('mlp_model.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test = np.loadtxt('data/test.csv', dtype='string', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "elt_list = []\n",
    "for t in train:\n",
    "    elt_list.append(float(t[2]))\n",
    "elist = sorted(elt_list)\n",
    "global_median = elist[train.shape[0] / 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print tac[0]\n",
    "def get_artist_labels():\n",
    "    d = {}\n",
    "    for i in range(len(tac)):\n",
    "        if tac[i][1] not in d:\n",
    "            d[train[i][1]] = int(tac[i][1])\n",
    "    return d\n",
    "art_labels = get_artist_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "2280000\n",
      "2290000\n",
      "2300000\n",
      "2310000\n",
      "2320000\n",
      "2330000\n",
      "2340000\n",
      "2350000\n",
      "2360000\n",
      "2370000\n",
      "2380000\n",
      "2390000\n",
      "2400000\n",
      "2410000\n",
      "2420000\n",
      "2430000\n",
      "2440000\n",
      "2450000\n",
      "2460000\n",
      "2470000\n",
      "2480000\n",
      "2490000\n",
      "2500000\n",
      "2510000\n",
      "2520000\n",
      "2530000\n",
      "2540000\n",
      "2550000\n",
      "2560000\n",
      "2570000\n",
      "2580000\n",
      "2590000\n",
      "2600000\n",
      "2610000\n",
      "2620000\n",
      "2630000\n",
      "2640000\n",
      "2650000\n",
      "2660000\n",
      "2670000\n",
      "2680000\n",
      "2690000\n",
      "2700000\n",
      "2710000\n",
      "2720000\n",
      "2730000\n",
      "2740000\n",
      "2750000\n",
      "2760000\n",
      "2770000\n",
      "2780000\n",
      "2790000\n",
      "2800000\n",
      "2810000\n",
      "2820000\n",
      "2830000\n",
      "2840000\n",
      "2850000\n",
      "2860000\n",
      "2870000\n",
      "2880000\n",
      "2890000\n",
      "2900000\n",
      "2910000\n",
      "2920000\n",
      "2930000\n",
      "2940000\n",
      "2950000\n",
      "2960000\n",
      "2970000\n",
      "2980000\n",
      "2990000\n",
      "3000000\n",
      "3010000\n",
      "3020000\n",
      "3030000\n",
      "3040000\n",
      "3050000\n",
      "3060000\n",
      "3070000\n",
      "3080000\n",
      "3090000\n",
      "3100000\n",
      "3110000\n",
      "3120000\n",
      "3130000\n",
      "3140000\n",
      "3150000\n",
      "3160000\n",
      "3170000\n",
      "3180000\n",
      "3190000\n",
      "3200000\n",
      "3210000\n",
      "3220000\n",
      "3230000\n",
      "3240000\n",
      "3250000\n",
      "3260000\n",
      "3270000\n",
      "3280000\n",
      "3290000\n",
      "3300000\n",
      "3310000\n",
      "3320000\n",
      "3330000\n",
      "3340000\n",
      "3350000\n",
      "3360000\n",
      "3370000\n",
      "3380000\n",
      "3390000\n",
      "3400000\n",
      "3410000\n",
      "3420000\n",
      "3430000\n",
      "3440000\n",
      "3450000\n",
      "3460000\n",
      "3470000\n",
      "3480000\n",
      "3490000\n",
      "3500000\n",
      "3510000\n",
      "3520000\n",
      "3530000\n",
      "3540000\n",
      "3550000\n",
      "3560000\n",
      "3570000\n",
      "3580000\n",
      "3590000\n",
      "3600000\n",
      "3610000\n",
      "3620000\n",
      "3630000\n",
      "3640000\n",
      "3650000\n",
      "3660000\n",
      "3670000\n",
      "3680000\n",
      "3690000\n",
      "3700000\n",
      "3710000\n",
      "3720000\n",
      "3730000\n",
      "3740000\n",
      "3750000\n",
      "3760000\n",
      "3770000\n",
      "3780000\n",
      "3790000\n",
      "3800000\n",
      "3810000\n",
      "3820000\n",
      "3830000\n",
      "3840000\n",
      "3850000\n",
      "3860000\n",
      "3870000\n",
      "3880000\n",
      "3890000\n",
      "3900000\n",
      "3910000\n",
      "3920000\n",
      "3930000\n",
      "3940000\n",
      "3950000\n",
      "3960000\n",
      "3970000\n",
      "3980000\n",
      "3990000\n",
      "4000000\n",
      "4010000\n",
      "4020000\n",
      "4030000\n",
      "4040000\n",
      "4050000\n",
      "4060000\n",
      "4070000\n",
      "4080000\n",
      "4090000\n",
      "4100000\n",
      "4110000\n",
      "4120000\n",
      "4130000\n",
      "4140000\n",
      "4150000\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "soln_file = 'data/mlp_real.csv'\n",
    "\n",
    "with open('data/test.csv', 'r') as test_fh:\n",
    "# with open('data/train.csv', 'r') as test_fh:\n",
    "    test_reader = csv.reader(test_fh, delimiter=',', quotechar='\"')\n",
    "    next(test_reader, None)\n",
    "    \n",
    "#     print test.shape\n",
    "\n",
    "    with open(soln_file, 'w') as soln_fh:\n",
    "        soln_csv = csv.writer(soln_fh,\n",
    "                              delimiter=',',\n",
    "                              quotechar='\"',\n",
    "                              quoting=csv.QUOTE_MINIMAL)\n",
    "        soln_csv.writerow(['Id', 'plays'])\n",
    "        \n",
    "        count = 0\n",
    "        for row in test_reader:\n",
    "            id     = row[0]\n",
    "            user   = row[1]\n",
    "            artist = row[2]\n",
    "            \n",
    "            if artist in art_labels and user in prof_dict:\n",
    "                    p = prof_dict[user]\n",
    "\n",
    "                    temp_x = []\n",
    "                    temp_x += make_one_hot(200, labels[p])\n",
    "                    temp_x += make_one_hot(50, art_labels[artist])\n",
    "\n",
    "                    if profiles[p][1] == 'f':\n",
    "                        temp_x += make_one_hot(3, 0)\n",
    "                    elif profiles[p][1] == 'm':\n",
    "                        temp_x += make_one_hot(3, 1)\n",
    "                    else:\n",
    "                        temp_x += make_one_hot(3, 2)\n",
    "\n",
    "                    if profiles[p][2] == '':\n",
    "                        temp_x += make_one_hot(121, 120)\n",
    "                    else:\n",
    "                        temp_x += make_one_hot(121, int(profiles[p][2]))\n",
    "\n",
    "                    temp_x += nation_dict[profiles[p][3]]\n",
    "\n",
    "                    x = np.array([temp_x])\n",
    "\n",
    "                    soln_csv.writerow([id, MLP.predict(x)])\n",
    "            else:\n",
    "                soln_csv.writerow([id, global_median + 120.0])\n",
    "                \n",
    "            if count % 10000 == 0:\n",
    "                print count\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
