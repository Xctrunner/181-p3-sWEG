\documentclass[11pt]{article}

\usepackage{common}
\title{Practical 3: Music Recommendation}
\author{Ethan Alley, Grigory Khimulya, Walter Martin \\ 
alley@college, khimulya@college, wmartin@college }
\begin{document}


\maketitle{}


\section{Technical Approach}

We tried several approaches.
\begin{itemize}
	\item One-hot encoding everything, user, artist, and demographics, and running a NN regression on that
	\item Clustering users based on how many plays they had given each artist
	\item Clustering artists based on the demographics of their listeners
	\item Clustering users based on how many plays that had given each artist cluster (which implicitly includes demographic information)
	\item Combinations of different types of averages (non parametric)
	\item Running a NN regression on the one-hot encoding of this user cluster, the artist cluster, and user demographic information for each training example
\end{itemize}

\noindent The first attempt at one-hot encoding was done with the out-of-the-box sklearn MLPRegressor. 

The first attempt at user clustering was done with a similar strategy to the artist clustering described immediately below. Ultimately, the $\sim200000$ users being clustered using $2000$ dimensional vectors was not computation feasible.

Artist clustering was done with Walter's $k$-means implementation from the previous theory pset. To start, a size $2000 \times n$ matrix was initialized, where $n$ is the size of the one-hot encoding of the demographics (age was limited to be in $[0, 119]$, which required some scrubbing ahead of time). For each element in the training data, the concatenated result of all of the one-hot vectors for that user was added to the row of the matrix corresponding to that artist. At the end, each row was divided by the total number of times that artist appeared in the training data (which emphasizes which demographics like the artist, not how popular the artist is). $k$-means with $k = 50$ was run to produce centroids which could then be used to assign examples to a cluster.

The second attempt at user clustering was done the same way as above. A size $\sim 200000 \times 50$ matrix was initialized. For each training example, the artist cluster was identified, and the number of plays added to the element of the matrix corresponding to the appropriate user and artist cluster. Here, $k$ was chosen to be $200$. 

Averages and combinations involved getting averages and medians globally as well as for individual artists and users. Various linear combinations of these values were tested with a held-out validation set.

The second attempt at neural network regression was done much the same way as the first, with sklearn's MLPRegressor, but instead of gigantic one-hot encodings for users and artists, the new feature vector was composed of $200 + 50 + n$ elements, where $n$ is once again the size of the one-hot encoding of the gender, age, and nation demographics. This regression was done on about a quarter of the data.


\section{Results}

We submitted predictions. The first two were simple submissions with linear combinations of means and medians - these were not better than the user median baseline. The third was a dud. It was meant to be the result of the dual-clustered NN, but due to a bug in the construction of the submission CSV, the values inside were garbage. \\

\noindent Results for each method attempted: 
\begin{itemize}
	\item Our first attempt at regression was terminated early - we have no results.
	\item The original user clustering was also terminated early.
	\item Without context, this means very little, but the result of the objective function on artist clustering using the $\ell_2$ norm was $244.46$, where there were $2000$ artists and the values of each element of their features were between $0$ and $1$.
	\item The result of the objective function on user clustering was not recorded.
	\item Combinations of averages:
	\begin{table}[h]
		\centering
		\begin{tabular}{llr}
			\toprule
			Model &  & Error \\
			\midrule
			Global Median & & 312462 \\
			Artist Median & & 307826 \\
			Global Median + 120 & & 295602 \\
			Artist Mean & & 291215\\
			User Median & & 175461  \\
			User Mean & &168582 \\
			.5 * UMean + .5 * UMed & & 165047 \\
			.7 * UMean + .3 * UMed & & 164788 \\
			.6 * UMean + .4 * UMed & & 164639 \\
			.6 * UMean + .4 * UMed + 20 & & 163995 \\
			\bottomrule
		\end{tabular}
		\caption{\label{tab:results} MSE for each model.}
	\end{table}

	\item Using the same error metric as the table above, the quarter-data dual-cluster NN regression produced an error of $144942$, better than anything else we'd seen.
\end{itemize}


\section{Discussion} 

\begin{itemize}
	\item We first attempted the large feature vector implementation because in theory, all the information available to us would be encoded that way, and we wanted to see if it was possible to do a simple regression. Because it wasn't, we started looking at clustering.
	\item Our first thought was that the largest factor in the aggressively sized feature vector was the one-hot encoding of the users, it being over $200000$ elements long. As such, we decided that crunching the user features down with clustering would be the most effective way to make running a neural net feasible. This may have been true, but at least using Walter's $k$-means implementation from last pset (he forgot that sklearn has a $k$-means package), this was also taking far too long.
	\item As a result, we decided to make the problem even smaller. Our reasoning was that if we could cluster the $2000$ artists down to $50$, we would stand a better chance clustering users with scaled-down feature vectors. This was effective. Which the handmade $k$-means may not have been fast, it got the job done, providing a reasonable spread across $50$ centroids.
	\item Clustering users was still slow, but much more feasible than before. Once put on a faster machine, this went according to plan.
	\item The averages and medians were tested out while waiting for other code to run. The impetus for this was the competition baseline, to see if we could improve on it with some small tweaks. For the most part, we couldn't. We tried a variety of approaches, including means and medians based for global data, user data, and artist data. Out of these on their own, the user mean was the most effective, but turned out to improve its performance on our cross validation data when combined with lower-weighted median data. It didn't seem that combination with any kind of artist mean helped. Finally, we experimented with also adding a bias term, which did make a small improvement in error. None of these models passed the competition baseline, however.
	\item The neural network made a slight improvement after a few pernicious bugs were fixed. We didn't have enough time to do good cross-validation to optimize hyper-parameters; this would no doubt have benefited us, as would trying other models *cough* \textbf{random forest} *cough*. Given more time, this is an area we would have started putting more emphasis on.
\end{itemize}

\end{document}

